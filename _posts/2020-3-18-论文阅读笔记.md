---
title: GhostNet论文阅读笔记
commentable: flase
Edit: 2020-3-18
mathjax: true
mermaid: true
tags: CVPR2020 CV 网络模块设计
categories: 网络模块设计
description: 这是CVPR2020的一篇论文，关于设计轻量化网络模块的，可以在现有模型中即插即用。like a [link](https://code-conquer.github.io).
---

**GhostNet: More Features from Cheap Operations**

论文地址：<a href="https://arxiv.org/abs/1911.11907">https://arxiv.org/abs/1911.11907 </a>

ps : 这篇论文出自**华为诺亚方舟实验室**

# 提出问题

由于内存和计算资源的限制，在嵌入式设备上部署CNN模型是非常困难的。通过作者的研究发现，在CNN中，存在大量相似的特征图，这些相似的特征图也是通过计算量很大的卷积运算得到。那么，能不能用一种__性价比较高__的方式来得到相似的特征图呢？这样可以极大减少模型的计算量。

下图是作者可视化CNN过程中的特征图，发现有3对相似的特征对。

![image-20200314134002843](C:\Users\6\AppData\Roaming\Typora\typora-user-images\image-20200314134002843.png)



# 解决问题 

__提出了新颖的Ghost module ，这个module可以用很少参数来产生更多的特征图__

__先回顾一下传统卷积__：

1.假设输入数据$X\in \mathbb{R}^{c\times h\times w}$ ，$c$ 是输入数据的通道数，$h$ 和 $w$ 是输入数据的高和宽.

2.则卷积操作可以表示为：
$$
Y=X*f+b
$$

$ Y \in \mathbb{R}^{h'\times w'\times n}$ 表示输出的特征，其中n表示特征的通道数，$ h'$ 和$ w' $ 分别表示输入特征图的高度和宽度 . $f \in \mathbb{R}^{c \times k\times k \times n}$ 表示这一层的卷积核，$k*k$ 表示卷积核的大小.  $*$ 表示卷积运算，$ b $  代表偏置.

3.由此可知这一层卷积运算的运算量 __FLOPs__   为 $n \cdot h' \cdot w' \cdot c \cdot k \cdot k $ 



__Ghost module:__

__思想__ ：

卷积得到的特征中，有一些固有特征(__intrinsic feature__ ),还有一些和这些固有特征相似的特征，称之为冗余特征（__ghosts__），这些冗余特征没有必要通过消耗大量的计算量和参数实现，可以在固有特征上做不同的线性变换（__低计算量低参数的方式__ ）得到。

__具体实现:__ 

1.输入数据和上述一样，假设卷积过后得到的特征中，固有特征数量为 $m$ ，其中 $ m \leq n$  

2.则输出特征为 $ Y' \in  \mathbb{R}^{h' \times w'\times m}$  ,卷积操作表示为: 
$$
Y' = X * f'
$$
$f' \in \mathbb{R}^{c \times k \times k \times m}$ 为使用的卷积核,为了方便省略了偏置项.

3.为了得到n个特征，使用一系列  **cheap**  的线性运算作用在$Y'$ 的每一个特征上， 每一个特征生成 $s$ 个 __ghost features__  .具体公式如下：
$$
y_{ij} = \Phi_{i,j}(y_{i}'), \forall i =1,...,m,  j=1,...,s,
$$
$y'$ 表示第i个固有的特征图，$ y' \in Y' $ ,$\Phi_{i,j}$ 表示生成第$i$ 个固有特征的第$j$ 个（最后一个除外） __ghost feature__ 的线性函数.也就是说$y_i'$ 能够有一个或者多个 __ghost feature maps__  $\{y_{ij}\}_{j=1}^{s}$ ，$\Phi{i,s}$ 是每个固有特征的__identity mapping__ (也就是不做任何变换，保持原有特征) .由此可知:$ n = m \cdot s$ ,得到的n个特征图为$Y = [y_{11},y_{12},...,y_{ms}]$ ，整个流程如图2所示：

![image-20200318164728383](C:\Users\6\AppData\Roaming\Typora\typora-user-images\image-20200318164728383.png)



__Analysis on Complexities:__ 

假设Ghost module 包含一个__identity mapping__ 和 $m \cdot(s-1)=\frac{n}{s}\cdot(s-1)$  个线性操作，每个线性操作的核大小为$d \times d$ ,则加速比率$r_s$为：

$$
r_{s}=\displaystyle \frac{n \cdot h' \cdot w' \cdot c \cdot k \cdot k}{\frac{n}{s} \cdot h' \cdot w' \cdot c \cdot k\cdot k+(s-1) \cdot \frac{n}{s} \cdot h' \cdot w'\cdot d\cdot d}
$$

$$
=\frac{c\cdot k\cdot k}{\frac{1}{s}\cdot c\cdot k\cdot k +\frac{s-1}{s} \cdot d\cdot d}\approx \frac{s \cdot c}{s+c-1} \approx s.
$$
注:$d \times d$ 的大小和$k \times k$  的大小相似,并且$s \ll c$ .

压缩比率为:
$$
r_{c}=\displaystyle \frac{n\cdot c\cdot k\cdot k }{\frac{n}{s} \cdot c\cdot k\cdot k +\frac{s-1}{s} \cdot d\cdot d }\approx \frac{s\cdot c}{s+c-1} \approx s
$$


__Ghost Bottlenecks:__

![image-20200318175711185](C:\Users\6\AppData\Roaming\Typora\typora-user-images\image-20200318175711185.png)



# 实验 & 结论

## 实验 

![image-20200318190702060](C:\Users\6\AppData\Roaming\Typora\typora-user-images\image-20200318190702060.png)

​       论文对图1的ghost pair进行了不同核大小的线性变化测试，将左图作为输出右图作为输入训练depthwise卷积，然后使用训练的结果对左图进行变换，计算其变换后与右图的MSE。可以看到，不同的核大小下差值都很小，说明线性变换是有效的，而且核大小的影响不大，所以用核固定为d的depthwise卷积来进行公式3计算.

![image-20200318190812587](C:\Users\6\AppData\Roaming\Typora\typora-user-images\image-20200318190812587.png)

将VGG的卷积层替换成Ghost模块进行超参数测试，表3的 $s=2$  ,表4的$ d=3$ 

![image-20200318190957761](C:\Users\6\AppData\Roaming\Typora\typora-user-images\image-20200318190957761.png)

__可以看到使用Ghost模块不仅比其它压缩方法更能降低模型的体量，也最能保持模型准确率.__

![image-20200318191133929](C:\Users\6\AppData\Roaming\Typora\typora-user-images\image-20200318191133929.png)

 __对Ghost模块产生的特征进行了可视化，尽管从固有特征线性变换而来，但还是有明显的差异，说明线性变换足够灵活.__ 

![image-20200318191319061](C:\Users\6\AppData\Roaming\Typora\typora-user-images\image-20200318191319061.png)

**在大型网络上使用Ghost模块，压缩效果和准确率依然很不错 .**



## 结论

为了减少神经网络的计算消耗，论文提出__Ghost模块__ 来构建高效的网络结果。该模块将原始的卷积层分成两部分，先使用更少的卷积核来生成少量固有特征图，然后通过简单的线性变化操作来进一步高效地生成ghost特征图。从实验来看，对比其它模型，GhostNet的压缩效果最好，且准确率保持也很不错，论文思想十分值得参考与学习.